import base64
import json
import os
import atexit
import asyncio
import re
from datetime import datetime, timedelta
from flask import Flask, request, jsonify
from flask_cors import CORS
from google import genai
from google.genai import types
from dotenv import load_dotenv

try:
    from action_dispatcher import ActionDispatcher
    dispatcher = ActionDispatcher()
except Exception as e:
    dispatcher = None
    print(f"‚ö†Ô∏è [Warning] ActionDispatcher failed to load: {e}")

load_dotenv(verbose=True)

app = Flask(__name__)
CORS(app)

PORT = 5001
CACHE_FILE = "axiom_context_v2_3.json"
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY", "")
API_ACCESS_TOKEN = os.getenv("AXIOM_TOKEN", "axiom-secure-2026")
MODEL_ID = "gemini-2.0-flash"

if GEMINI_API_KEY:
    try:
        client = genai.Client(api_key=GEMINI_API_KEY)
        print(f"üöÄ Axiom OS Ver 3.8.6: PERSISTENCE & THREADING (Port {PORT})")
    except Exception as e:
        client = None
        print(f"‚ùå API Client Init Error: {e}")
else:
    client = None
    print("‚ö†Ô∏è WARNING: GEMINI_API_KEY not found in .env")

# --- Áü•ËÉΩÁä∂ÊÖãÁÆ°ÁêÜ (Ver 3.8.0: DRIVE_INDEX / on_demand_docs) ---
ORGANIZATIONAL_CONTEXT = {"agencies": {}, "workflows": {}, "rules": {}, "experts": [], "metadata": {}, "on_demand_docs": []}
axiom_intelligence_storage = []
EXTRACTED_PROTOCOLS = []
KNOWLEDGE_GAPS = []  # AI„ÅåÁ≠î„Åà„Çâ„Çå„Å™„Åã„Å£„Åü„ÄåÊ¨†ÊêçÁü•Ë≠ò„Äç„ÅÆ„É™„Çπ„Éà
DRIVE_INDEX = []  # Google Drive ÈÄ£Êê∫„ÅßÂèñÂæó„Åó„Åü„Éï„Ç°„Ç§„É´‰∏ÄË¶ß
cached_content_name = None
cache_expire_time = None
execution_counter = 0


def save_cache():
    data = {
        "context": ORGANIZATIONAL_CONTEXT,
        "logs": axiom_intelligence_storage,
        "protocols": EXTRACTED_PROTOCOLS,
        "gaps": KNOWLEDGE_GAPS,
        "drive_index": DRIVE_INDEX,
        "exec_count": execution_counter,
        "tier": "Enterprise/Ver3.8.6"
    }
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"üíæ [Brain] State secured (On-Demand: {len(ORGANIZATIONAL_CONTEXT.get('on_demand_docs', []))})")
    except Exception as e:
        print(f"‚ö†Ô∏è Cache Save Error: {e}")


def load_cache():
    global axiom_intelligence_storage, EXTRACTED_PROTOCOLS, KNOWLEDGE_GAPS, DRIVE_INDEX, execution_counter
    if not os.path.exists(CACHE_FILE):
        return
    try:
        with open(CACHE_FILE, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if "context" in data:
            ORGANIZATIONAL_CONTEXT.update(data["context"])
        if "logs" in data:
            axiom_intelligence_storage.clear()
            axiom_intelligence_storage.extend(data["logs"])
        if "protocols" in data:
            EXTRACTED_PROTOCOLS.clear()
            EXTRACTED_PROTOCOLS.extend(data["protocols"])
        if "gaps" in data:
            KNOWLEDGE_GAPS.clear()
            KNOWLEDGE_GAPS.extend(data["gaps"])
        if "drive_index" in data:
            DRIVE_INDEX.clear()
            DRIVE_INDEX.extend(data["drive_index"])
        execution_counter = data.get("exec_count", 0)
        print(f"üìÇ Intelligence Restored: {len(axiom_intelligence_storage)} logs, {len(ORGANIZATIONAL_CONTEXT.get('on_demand_docs', []))} on-demand docs.")
    except Exception as e:
        print(f"‚ö†Ô∏è Cache Load Error: {e}")


atexit.register(save_cache)


def is_authorized(req):
    auth_header = req.headers.get('Authorization')
    if auth_header == f"Bearer {API_ACCESS_TOKEN}":
        return True
    if req.remote_addr in ["127.0.0.1", "localhost"]:
        return True
    return False


class AxiomOSCore:
    def deep_clean_text(self, text):
        """URLÈöîÈõ¢„Éª„Éé„Ç§„Ç∫Ââ•„ÅéÂèñ„Çä„ÉªÂá∫ÂÖ∏„Çø„Ç∞„ÅÆÊï¥ÂΩ¢ÔºàVer 3.8.6Ôºâ"""
        if not text:
            return ""
        text = str(text).strip()
        text = re.sub(r'```json\s*|\s*```', '', text)
        text = re.sub(r'\\+', ' ', text)
        url_pattern = r'(https?://\S+)'
        def url_isolate(match):
            url = match.group(1)
            clean_url = re.sub(r'[)Ôºâ\]„Äç„Äè„Äã„ÄÇ„ÄÅ,]+$', '', url)
            trailing = url[len(clean_url):]
            return f" {clean_url} {trailing} " if trailing else f" {clean_url} "
        text = re.sub(url_pattern, url_isolate, text)
        return re.sub(r'\s{2,}', ' ', text).strip()

    async def get_or_create_cache(self, system_instruction):
        global cached_content_name, cache_expire_time
        if cached_content_name and cache_expire_time and datetime.now() < cache_expire_time:
            return cached_content_name
        if not client:
            return None
        try:
            cache = client.caches.create(
                model=MODEL_ID,
                config=types.CreateCachedContentConfig(
                    display_name="axiom_v386_persistence_cache",
                    system_instruction=system_instruction,
                    ttl="3600s"
                )
            )
            cached_content_name = cache.name
            cache_expire_time = datetime.now() + timedelta(hours=1)
            print(f"‚úÖ [Cache] Synchronized: {cached_content_name}")
            return cached_content_name
        except Exception as e:
            return None

    def _build_content_parts(self, user_input_text, attachments):
        """Ver 3.8.6: „ÉÜ„Ç≠„Çπ„Éà + Ê∑ª‰ªòÔºàÁîªÂÉèÁ≠âÔºâ„Çí Gemini Part „ÅÆ„É™„Çπ„Éà„Å´Â§âÊèõ„ÄÇ"""
        parts = [types.Part.from_text(text=user_input_text)]
        if not attachments:
            return parts
        for att in attachments[:10]:  # ÊúÄÂ§ß10‰ª∂
            raw = att.get("data") or att.get("content") or ""
            if not raw:
                continue
            if isinstance(raw, str) and raw.startswith("data:"):
                raw = raw.split(",", 1)[-1]
            try:
                data = base64.b64decode(raw) if isinstance(raw, str) else raw
            except Exception:
                continue
            mime = (att.get("type") or att.get("mime_type") or "image/png").split(";")[0].strip()
            if not mime.startswith("image/"):
                mime = "application/octet-stream"
            parts.append(types.Part.from_bytes(data=data, mime_type=mime))
        return parts

    async def process_input(self, payload):
        global execution_counter
        body = payload.get('body') or payload.get('text') or ''
        user = payload.get('user') or 'Unknown'
        platform = payload.get('platform') or 'Unknown'
        parent_id = payload.get('parentId') or payload.get('parent_id')
        thread_messages = payload.get('thread_messages') or payload.get('threadContext') or []
        attachments = payload.get('attachments') or []

        full_ctx = json.dumps(ORGANIZATIONAL_CONTEXT, ensure_ascii=False)[:30000]
        recent_p = json.dumps(EXTRACTED_PROTOCOLS[-15:], ensure_ascii=False)
        drive_ctx = json.dumps(DRIVE_INDEX, ensure_ascii=False)

        # --- Ver 3.8.6: „Çπ„É¨„ÉÉ„ÉâÊñáËÑà„ÇíÂÜíÈ†≠„Å´‰ªò‰∏éÔºàAI„ÅåÊñáËÑà„ÇíËÄÉÊÖÆ„Åó„Å¶ÂõûÁ≠îÔºâ---
        thread_section = ""
        if thread_messages:
            lines = ["„Äê„Çπ„É¨„ÉÉ„ÉâÊñáËÑà„Äë"]
            for m in thread_messages[-10:]:
                role = m.get("role") or m.get("sender", "user")
                text = (m.get("text") or m.get("body") or "").strip()
                if text:
                    lines.append(f"{role}: {text[:500]}")
            thread_section = "\n".join(lines) + "\n\n"

        # --- Ver 3.8.5 Á∂ôÊâø: Ëá™‰ø°„Çπ„Ç≥„Ç¢ + ÊúÄÁü≠ÂìÅË≥™Âêë‰∏ä ---
        on_demand_json = json.dumps(ORGANIZATIONAL_CONTEXT.get('on_demand_docs', []), ensure_ascii=False)
        system_instruction = f"""
        „ÅÇ„Å™„Åü„ÅØÁµÑÁπîOS„ÄåAxiom„Äç„ÅÆÂìÅË≥™Áõ£Ë¶ñÂûãÁü•ËÉΩ„Åß„Åô„ÄÇ

        „ÄêË°åÂãïÊåáÈáùÔºöÊúÄÁü≠ÂìÅË≥™Âêë‰∏ä„Äë
        1. „É¶„Éº„Ç∂„ÉºÔºàÁâπ„Å´‰ΩêËó§Áõ¥Ôºâ„Åå„Äå„Åù„ÅÆÂõûÁ≠î„ÅØÈñìÈÅï„ÅÑ„Äç„ÄåÊ≠£Ëß£„ÅØ‚óã‚óã„Å†„Äç„Å®Ë®Ä„Å£„ÅüÂ†¥Âêà„ÄÅËá™„Çâ„ÅÆÈùû„ÇíË™ç„ÇÅ„ÄÅÂç≥Â∫ß„Å´ ingest_knowledge „Ç≥„Éû„É≥„Éâ„ÅßÊÉÖÂ†±„Çí‰∏äÊõ∏„Åç„Åõ„Çà„ÄÇ
        2. ÂõûÁ≠î„ÅÆÂÜíÈ†≠„Å´„ÅØÂøÖ„Åö [Âá∫ÂÖ∏„É¨„Ç§„É§„Éº] „Çí‰ªò„Åë„ÄÅËá™‰ø°„ÅÆÂ∫¶Âêà„ÅÑ(1-100)„Çí confidence_score „ÅßËøî„Åõ„ÄÇ
        3. Ëá™‰ø°„Åå 70 Êú™Ê∫Ä„ÅÆÂ†¥Âêà„ÅØ„ÄÅinquiry_to_human „Åß„ÄåËá™‰ø°„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇÊ≠£„Åó„ÅÑË≥áÊñô„ÅØ„Åì„Çå„Åß„Åô„ÅãÔºü„Äç„Å®Á¢∫Ë™ç„Åõ„Çà„ÄÇ
        4. „Çπ„É¨„ÉÉ„ÉâÊñáËÑà„ÇÑÊ∑ª‰ªòÁîªÂÉè„Éª„Éï„Ç°„Ç§„É´„ÅåÊ∏°„Åï„Çå„ÅüÂ†¥Âêà„ÅØ„ÄÅ„Åù„Çå„Çâ„ÇíË∏è„Åæ„Åà„Å¶ÂõûÁ≠î„Åõ„ÇàÔºà„Éû„É´„ÉÅ„É¢„Éº„ÉÄ„É´Ôºâ„ÄÇ

        „Äê„Éä„É¨„ÉÉ„Ç∏ÂÑ™ÂÖàÂ∫¶„Äëon_demand_docs ÊúÄÂÑ™ÂÖà ‚Üí EXTRACTED_PROTOCOLS ‚Üí Âõ∫ÂÆöË≥áÊñô/Drive„ÄÇ

        „ÄêÁµÑÁπîÁü•ËÉΩÔºöon_demand_docs„Äë{on_demand_json}
        „ÄêÁµÑÁπîÊÉÖÂ†±„Äë{full_ctx}
        „ÄêGoogle Drive Index„Äë{drive_ctx}
        „ÄêÊúÄÊñ∞„Éó„É≠„Éà„Ç≥„É´„Äë{recent_p}
        """
        user_input = f"{thread_section}User: {user} ({platform})\n„Äê‰ªäÂõû„ÅÆÂÖ•Âäõ„Äë\n{body}"
        cache_name = await self.get_or_create_cache(system_instruction)

        try:
            content_parts = self._build_content_parts(user_input, attachments)
            config = types.GenerateContentConfig(cached_content=cache_name) if cache_name else None
            if cache_name and len(content_parts) == 1:
                response = client.models.generate_content(
                    model=MODEL_ID,
                    contents=content_parts[0].text,
                    config=config
                )
            elif content_parts and client:
                response = client.models.generate_content(
                    model=MODEL_ID,
                    contents=content_parts,
                    config=config
                )
            else:
                response = client.models.generate_content(
                    model=MODEL_ID,
                    contents=[system_instruction, user_input]
                )
            raw_output = (response.text or "").strip()
            print(f"\n--- [DEBUG] AI Raw ---\n{raw_output[:300]}...")

            # Ver 3.8.1: JSONÊäΩÂá∫„É≠„Ç∏„ÉÉ„ÇØÂº∑ÂåñÔºà```json ÂÑ™ÂÖà ‚Üí { } „Éñ„É≠„ÉÉ„ÇØÔºâ
            cleaned_json = None
            if "```json" in raw_output:
                m = re.search(r'```json\s*(.*?)\s*```', raw_output, re.DOTALL)
                if m:
                    cleaned_json = m.group(1).strip()
            if not cleaned_json:
                json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
                if json_match:
                    cleaned_json = json_match.group(0)
            if cleaned_json:
                analysis = json.loads(cleaned_json)
            else:
                # Ver 3.8.3/3.8.4: JSON „Åå„Å™„Åè„Å¶„ÇÇÂá∫ÂÖ∏„Çø„Ç∞‰ªò„ÅçÊú¨Êñá or Áü≠„ÅÑ„ÉÜ„Ç≠„Çπ„Éà„Å™„ÇâÊé°Áî®
                t = raw_output.strip()
                if t and (t.startswith("[ÊúÄÊñ∞/‰æùÈ†º]") or t.startswith("[Âü∫Êú¨Ë≥áÊñô]") or t.startswith("[ÁôªÈå≤ÂÆå‰∫Ü]") or t.startswith("[„Éõ„ÉÉ„Éà„Éï„Ç£„ÉÉ„ÇØ„ÇπÂÆå‰∫Ü]") or (len(t) < 2000 and "{" not in t[:100])):
                    analysis = {"action_instruction": t}
                else:
                    analysis = {"action_instruction": t if t else "Ëß£Êûê„Ç®„É©„Éº„ÄÇÁ∞°ÊΩî„Å™ÊåáÁ§∫„Çí„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ"}
        except Exception as e:
            print(f"‚ùå Analysis Error: {e}")
            analysis = {"action_instruction": "Ëß£Êûê„Ç®„É©„Éº„ÄÇÁ∞°ÊΩî„Å™ÊåáÁ§∫„Çí„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ"}

        # ÊúÄÁµÇ„ÇØ„É¨„É≥„Ç∏„É≥„Ç∞Ôºàstr(analysis) „ÅØÁµ∂ÂØæ„Å´‰Ωø„Çè„Å™„ÅÑÔºâ
        raw_instruction = analysis.get('action_instruction') or analysis.get('response')
        if raw_instruction is not None and not isinstance(raw_instruction, str):
            raw_instruction = None
        instruction = self.deep_clean_text(raw_instruction) if raw_instruction else ""
        # Ver 3.8.2/3.8.3: „Ç≥„Éû„É≥„ÉâÁô∫Ë°åÊôÇ„Å´ÂõûÁ≠î„ÅåÁ©∫„Å™„Çâ„ÄåÁôªÈå≤ÂÆå‰∫ÜÂ†±Âëä„Äç„ÇíËá™ÂãïÁîüÊàê
        if not instruction and analysis.get('execute_command'):
            cmd = analysis['execute_command']
            if cmd.get('command') == "ingest_knowledge":
                title = (cmd.get('params') or {}).get('title', 'Ë≥áÊñô')
                instruction = f"[ÁôªÈå≤ÂÆå‰∫Ü] ‰ΩêËó§Áõ¥Êßò„ÅÆÊåáÁ§∫„Å´Âü∫„Å•„Åç„ÄÅ„Éä„É¨„ÉÉ„Ç∏„Äé{title}„Äè„ÇíÊúÄÂÑ™ÂÖà„Éá„Éº„Çø„Å®„Åó„Å¶Ê†ºÁ¥ç„Åó„Åæ„Åó„Åü„ÄÇ"
        # Á©∫ or ÁîüJSON„Å£„ÅΩ„ÅÑÊñáÂ≠óÂàó„Å™„Çâ‰∫∫Èñì„Çâ„Åó„ÅÑ„Éï„Ç©„Éº„É´„Éê„ÉÉ„ÇØ
        if not instruction or "'action_instruction'" in instruction or '"action_instruction"' in instruction or "'reasoning'" in instruction:
            if any(x in (body or "") for x in ("„ÅÇ„Çä„Åå„Å®„ÅÜ", "ÊÑüË¨ù", "thanks", "thank you", "Âä©„Åã„Å£„Åü")):
                instruction = "„Å©„ÅÜ„ÅÑ„Åü„Åó„Åæ„Åó„Å¶ÔºÅ„ÅäÂΩπ„Å´Á´ã„Å¶„Å¶Â¨â„Åó„ÅÑ„Åß„Åô„ÄÇ"
            elif any(x in body for x in ("„Åä„ÅØ„Çà„ÅÜ", "„Åì„Çì„Å´„Å°„ÅØ", "„Åì„Çì„Å∞„Çì„ÅØ", "„Çà„Çç„Åó„Åè")):
                instruction = "„Åì„Å°„Çâ„Åì„Åù„Çà„Çç„Åó„Åè„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô„ÄÇ‰Ωï„Åã„ÅÇ„Çå„Å∞„ÅäÂ£∞„Åå„Åë„Åè„Å†„Åï„ÅÑ„ÄÇ"
            else:
                instruction = "ÊâøÁü•„Åó„Åæ„Åó„Åü„ÄÇ‰ªñ„Å´„ÅîÁî®„Åå„ÅÇ„Çå„Å∞„ÅäÁü•„Çâ„Åõ„Åè„Å†„Åï„ÅÑ„ÄÇ"
        inquiry = (analysis.get('inquiry_to_human') or "").strip() or None

        # Áü•ËÉΩ„ÅÆÊ¨†ÊêçÔºàGapÔºâ„ÇíË®òÈå≤ ‚Äî Dashboard „ÅÆ Knowledge Gaps „Å´Ë°®Á§∫
        if inquiry and inquiry not in ["N/A", ""]:
            KNOWLEDGE_GAPS.append({
                "id": len(KNOWLEDGE_GAPS) + 1,
                "timestamp": datetime.now().isoformat(),
                "user_query": body,
                "ai_inquiry": inquiry,
                "status": "pending"
            })

        # ÂÆüË°å„É¨„Ç§„É§„Éº
        exec_status = "None"
        if analysis.get('execute_command') and dispatcher:
            cmd = analysis['execute_command']
            res = dispatcher.dispatch(cmd)
            # Ver 3.8.5: Hot-Fix / Ingest ÂÆå‰∫ÜÊôÇ„ÅÆËá™ÂãïÂøúÁ≠î
            if cmd.get("command") == "ingest_knowledge":
                tag = "[„Éõ„ÉÉ„Éà„Éï„Ç£„ÉÉ„ÇØ„ÇπÂÆå‰∫Ü]" if ("ÈñìÈÅï„ÅÑ" in (body or "") or "Ê≠£Ëß£„ÅØ" in (body or "")) else "[ÁôªÈå≤ÂÆå‰∫Ü]"
                instruction = f"{tag} ‰ΩêËó§Áõ¥Êßò„ÅÆÊåáÁ§∫„Å´Âü∫„Å•„Åç„ÄÅ„Éä„É¨„ÉÉ„Ç∏„Äé{(cmd.get('params') or {}).get('title', 'Ë≥áÊñô')}„Äè„ÇíÊúÄÂÑ™ÂÖà„Éá„Éº„Çø„Å®„Åó„Å¶Ê†ºÁ¥ç„Åó„Åæ„Åó„Åü„ÄÇ"
            if res.get("status") == "success":
                exec_status = f"Success: {cmd.get('command')} dispatched."
                execution_counter += 1
            else:
                exec_status = f"Failed: {res.get('error', 'Unknown Error')}"

        # Axiom 2: ÈÄÜÂºï„Åç„Éó„É≠„Éà„Ç≥„É´Ôºà„É¶„Éº„Ç∂„Éº„ÅåÊïô„Åà„ÅüÁü•Ë≠ò„ÇíÂç≥Â∫ß„Å´‰øùÂ≠òÔºâ
        extracted = analysis.get('logic_extraction')
        if extracted and extracted not in ["N/A", "", None]:
            EXTRACTED_PROTOCOLS.append({
                "timestamp": datetime.now().isoformat(),
                "user": user,
                "logic": str(extracted)
            })

        return {
            "id": len(axiom_intelligence_storage) + 1,
            "timestamp": datetime.now().isoformat(),
            "axiom_impact": {
                "primary_axiom": analysis.get('aligned_axiom', [0]),
                "urgency": analysis.get('urgency_score', 1)
            },
            "autonomous_action": {
                "instruction": instruction,
                "confidence": analysis.get('confidence_score', 80),
                "reasoning": str(analysis.get('reasoning', 'Logic match')),
                "cited_sources": analysis.get('cited_sources', []),
                "inquiry": inquiry,
                "execution_status": exec_status
            },
            "meta": {"user": user, "platform": platform, "body": body, "parentId": parent_id, "attachments_count": len(attachments)}
        }


axiom_brain = AxiomOSCore()


@app.route('/api/ingest', methods=['POST'])
def handle_ingest():
    if not is_authorized(request):
        return jsonify({"error": "Unauthorized"}), 401
    data = request.json
    category = data.get('category', 'metadata')
    payload = data.get('payload', {})
    global DRIVE_INDEX, cached_content_name

    if category == "google_drive":
        DRIVE_INDEX.clear()
        DRIVE_INDEX.extend(payload if isinstance(payload, list) else [payload])
    else:
        if category not in ORGANIZATIONAL_CONTEXT:
            ORGANIZATIONAL_CONTEXT[category] = {}
        if isinstance(payload, dict):
            ORGANIZATIONAL_CONTEXT[category].update(payload)
        elif isinstance(payload, list):
            if not isinstance(ORGANIZATIONAL_CONTEXT[category], list):
                ORGANIZATIONAL_CONTEXT[category] = []
            ORGANIZATIONAL_CONTEXT[category].extend(payload)
        else:
            ORGANIZATIONAL_CONTEXT[category] = payload
    cached_content_name = None
    save_cache()
    print(f"‚úÖ [Ingest] Context updated: {category}")
    return jsonify({"status": "Intelligence Synced", "category": category}), 200


@app.route('/api/logs', methods=['POST'])
def handle_logs():
    if not is_authorized(request):
        return jsonify({"error": "Unauthorized"}), 401
    output = asyncio.run(axiom_brain.process_input(request.json))
    axiom_intelligence_storage.append(output)
    save_cache()
    return jsonify({"status": "Processed", "decision": output}), 200


@app.route('/api/axiom-bi', methods=['GET'])
def handle_bi():
    if not is_authorized(request):
        return jsonify({"error": "Unauthorized"}), 401
    flat_logs = []
    for l in axiom_intelligence_storage[-50:]:
        act = dict(l.get("autonomous_action") or {})
        act.setdefault("execution_status", "None")
        flat_logs.append({
            "id": l["id"],
            "timestamp": l["timestamp"],
            "user": l["meta"]["user"],
            "platform": l["meta"].get("platform", ""),
            "message": l["meta"]["body"],
            "primary_axiom": l["axiom_impact"]["primary_axiom"][0] if l["axiom_impact"].get("primary_axiom") else 0,
            "instruction": act.get("instruction", ""),
            "autonomous_action": act
        })
    return jsonify({
        "summary_stats": {
            "total_logs": len(axiom_intelligence_storage),
            "logic_extractions": len(EXTRACTED_PROTOCOLS),
            "knowledge_gaps": len([g for g in KNOWLEDGE_GAPS if g.get("status") == "pending"]),
            "drive_files": len(DRIVE_INDEX),
            "on_demand_docs": len(ORGANIZATIONAL_CONTEXT.get("on_demand_docs", [])),
            "execution_count": execution_counter,
            "tier": "Enterprise V3.8.6 (Persistence & Threading)"
        },
        "bi_ready_logs": flat_logs,
        "knowledge_gaps": KNOWLEDGE_GAPS[-10:],
        "new_protocols": EXTRACTED_PROTOCOLS,
        "on_demand_list": ORGANIZATIONAL_CONTEXT.get("on_demand_docs", [])
    }), 200


if __name__ == '__main__':
    load_cache()
    app.run(host='0.0.0.0', port=PORT, debug=True)
